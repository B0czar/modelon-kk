{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo Ensemble para Previsão de Sucesso de Startups (Kaggle)\n",
        "\n",
        "Este notebook realiza o seguinte fluxo end-to-end:\n",
        "\n",
        "1. **Carregamento & Engenharia de Atributos** – leitura de `train.csv`/`test.csv`, criação de novas features.\n",
        "2. **Pré-processamento** – imputação, escalonamento e one-hot encoding em um `ColumnTransformer`.\n",
        "3. **Otimização de Hiperparâmetros** – busca em grade (GridSearchCV) para **RandomForest**, **GradientBoosting** e **ExtraTrees** visando maximizar *accuracy*.\n",
        "4. **Ensemble** – construção de um `VotingClassifier` *soft* com os três melhores modelos.\n",
        "5. **Avaliação** – *cross-validation* estratificada (5 folds) em todo o `train.csv` (não há *split* train/validation separados neste desafio). Meta: **accuracy ≥ 0.90**.\n",
        "6. **Treino Final & Submissão** – re-treino do ensemble com todos os dados de treino e geração de `submissions/submission.csv` para o `test.csv`.\n",
        "\n",
        "Somente bibliotecas permitidas (`numpy`, `pandas`, `scikit-learn`) são utilizadas, conforme regras do README.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração e imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregamento de dados e definição de X e y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape treino: (646, 33), Shape teste: (277, 32)\n",
            "Distribuição do target: {1: 418, 0: 228}\n",
            "Proporção classe 1: 0.647\n"
          ]
        }
      ],
      "source": [
        "# Carrega os dados\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "print(f\"Shape treino: {train.shape}, Shape teste: {test.shape}\")\n",
        "print(f\"Distribuição do target: {train['labels'].value_counts().to_dict()}\")\n",
        "print(f\"Proporção classe 1: {train['labels'].mean():.3f}\")\n",
        "\n",
        "X = train.drop('labels', axis=1)\n",
        "y = train['labels']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Engenharia de atributos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering Avançado\n",
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Features de tempo\n",
        "    df['funding_duration'] = df['age_last_funding_year'] - df['age_first_funding_year']\n",
        "    df['milestone_duration'] = df['age_last_milestone_year'] - df['age_first_milestone_year']\n",
        "    df['time_to_first_milestone'] = df['age_first_milestone_year'] - df['age_first_funding_year']\n",
        "\n",
        "    # Features de funding por round\n",
        "    df['funding_per_round'] = df['funding_total_usd'] / (df['funding_rounds'] + 1e-6)\n",
        "    df['milestones_per_round'] = df['milestones'] / (df['funding_rounds'] + 1e-6)\n",
        "\n",
        "    # Features de relacionamentos\n",
        "    df['relationships_per_round'] = df['relationships'] / (df['funding_rounds'] + 1e-6)\n",
        "    df['participants_per_round'] = df['avg_participants'] * df['funding_rounds']\n",
        "\n",
        "    # Features de funding por milestone\n",
        "    df['funding_per_milestone'] = df['funding_total_usd'] / (df['milestones'] + 1e-6)\n",
        "\n",
        "    # Features de estado (one-hot já existe, mas vamos criar features combinadas)\n",
        "    df['is_major_state'] = df['is_CA'] + df['is_NY'] + df['is_MA'] + df['is_TX']\n",
        "\n",
        "    # Features de categoria (one-hot já existe, mas vamos criar features combinadas)\n",
        "    df['is_tech_category'] = df['is_software'] + df['is_web'] + df['is_mobile']\n",
        "    df['is_business_category'] = df['is_enterprise'] + df['is_advertising'] + df['is_consulting']\n",
        "    df['is_consumer_category'] = df['is_gamesvideo'] + df['is_ecommerce']\n",
        "\n",
        "    # Features de funding rounds\n",
        "    df['has_multiple_rounds'] = (df['funding_rounds'] > 1).astype(int)\n",
        "    df['has_advanced_rounds'] = (df['has_roundC'] + df['has_roundD']).astype(int)\n",
        "\n",
        "    # Features de milestones\n",
        "    df['has_milestones'] = (df['milestones'] > 0).astype(int)\n",
        "    df['milestone_intensity'] = df['milestones'] / (df['age_last_funding_year'] - df['age_first_funding_year'] + 1e-6)\n",
        "\n",
        "    # Features de funding total (log scale)\n",
        "    df['log_funding_total'] = np.log1p(df['funding_total_usd'])\n",
        "\n",
        "    # Features de idade (log scale)\n",
        "    df['log_age_first_funding'] = np.log1p(df['age_first_funding_year'])\n",
        "    df['log_age_last_funding'] = np.log1p(df['age_last_funding_year'])\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FEATURE ENGINEERING\n",
            "Features originais: 32\n",
            "Features após engenharia: 51\n",
            "Features numéricas: 50\n",
            "Features categóricas: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFEATURE ENGINEERING\")\n",
        "X_engineered = create_features(X)\n",
        "test_engineered = create_features(test)\n",
        "\n",
        "print(f\"Features originais: {X.shape[1]}\")\n",
        "print(f\"Features após engenharia: {X_engineered.shape[1]}\")\n",
        "\n",
        "# Seleciona colunas numéricas e categóricas\n",
        "numeric_features = X_engineered.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_engineered.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Features numéricas: {len(numeric_features)}\")\n",
        "print(f\"Features categóricas: {len(categorical_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pré-processamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Buscando hiperparâmetros para: rf\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Otimização de hiperparâmetros\n",
        "# ================================\n",
        "\n",
        "# Grades de busca (pequenas para demo; ajuste conforme tempo disponível)\n",
        "param_grids = {\n",
        "    'rf': {\n",
        "        'model__n_estimators': [200, 400],\n",
        "        'model__max_depth': [10, 20, None],\n",
        "        'model__min_samples_split': [2, 5],\n",
        "        'model__min_samples_leaf': [1, 2],\n",
        "        'model__max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'gb': {\n",
        "        'model__n_estimators': [200, 400],\n",
        "        'model__learning_rate': [0.05, 0.1],\n",
        "        'model__max_depth': [3, 5]\n",
        "    },\n",
        "    'et': {\n",
        "        'model__n_estimators': [200, 400],\n",
        "        'model__max_depth': [10, None],\n",
        "        'model__min_samples_split': [2, 5],\n",
        "        'model__min_samples_leaf': [1, 2],\n",
        "        'model__max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "}\n",
        "\n",
        "base_models = {\n",
        "    'rf': RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
        "    'gb': GradientBoostingClassifier(random_state=42),\n",
        "    'et': ExtraTreesClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "best_estimators = {}\n",
        "cv_results = []\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in base_models.items():\n",
        "    print(f\"\\n>>> Buscando hiperparâmetros para: {name}\")\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    grid = GridSearchCV(pipe,\n",
        "                        param_grid=param_grids[name],\n",
        "                        scoring='accuracy',\n",
        "                        cv=cv,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=0)\n",
        "\n",
        "    grid.fit(X_engineered, y)\n",
        "    best_estimators[name] = grid.best_estimator_\n",
        "    cv_results.append({\n",
        "        'modelo': name,\n",
        "        'melhor_acc': grid.best_score_,\n",
        "        'melhor_params': grid.best_params_\n",
        "    })\n",
        "    print(f\"Melhor acc (CV): {grid.best_score_:.4f}\")\n",
        "\n",
        "cv_df = pd.DataFrame(cv_results).sort_values('melhor_acc', ascending=False)\n",
        "print(\"\\nResumo CV:\\n\", cv_df[['modelo', 'melhor_acc']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================================\n",
        "# Ensemble (VotingClassifier) & Avaliação\n",
        "# ==================================\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[(name, est) for name, est in best_estimators.items()],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble_pipe = voting_clf  # já contém preprocessor embutido em cada est.\n",
        "\n",
        "print(\"\\nValidando ensemble (5-fold CV)...\")\n",
        "acc_ensemble = cross_val_score(ensemble_pipe, X_engineered, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"Accuracy média ensemble: {acc_ensemble.mean():.4f} ± {acc_ensemble.std():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Treino final & submissão\n",
        "# ===========================\n",
        "\n",
        "print(\"\\nTreinando ensemble final em todo o treino...\")\n",
        "ensemble_pipe.fit(X_engineered, y)\n",
        "\n",
        "print(\"Gerando predições de teste e arquivo de submissão...\")\n",
        "\n",
        "test_pred = ensemble_pipe.predict(test_engineered)\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'labels': test_pred\n",
        "})\n",
        "\n",
        "sub_dir = 'submissions'\n",
        "import os\n",
        "os.makedirs(sub_dir, exist_ok=True)\n",
        "sub_path = os.path.join(sub_dir, 'submission.csv')\n",
        "sub.to_csv(sub_path, index=False)\n",
        "print(f\"Arquivo salvo em: {sub_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pré-processamento mais sofisticado\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"Pipeline de pré-processamento configurado:\")\n",
        "print(f\"- Features numéricas: {len(numeric_features)}\")\n",
        "print(f\"- Features categóricas: {len(categorical_features)}\")\n",
        "print(\"- Imputação: KNN para numéricas, moda para categóricas\")\n",
        "print(\"- Escalonamento: RobustScaler para numéricas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definição de modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelos individuais ajustados para maximizar a acurácia\n",
        "\n",
        "# Random Forest padrão\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,           # Mais árvores para melhor generalização\n",
        "    max_depth=15,               # Controla overfitting\n",
        "    min_samples_split=5,        # Evita divisões muito específicas\n",
        "    min_samples_leaf=2,         # Garante folhas com amostras suficientes\n",
        "    max_features='sqrt',        # Usa sqrt(n_features) para cada split\n",
        "    bootstrap=True,             # Bootstrap para diversidade\n",
        "    class_weight='balanced',    # Lida com desbalanceamento (64.7% classe 1)\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Gradient Boosting com hiperparâmetros ajustados\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=2500,      # Mais árvores\n",
        "    learning_rate=0.02,     # Menor taxa de aprendizado para maior precisão\n",
        "    max_depth=12,           # Aumenta profundidade\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    subsample=0.8,          # Mais amostras por árvore\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Extra Trees padrão\n",
        "et = ExtraTreesClassifier(\n",
        "    n_estimators=300,           # Mais árvores para melhor generalização\n",
        "    max_depth=20,               # Extra Trees pode ter mais profundidade (mais aleatório)\n",
        "    min_samples_split=2,        # Menor que RF (Extra Trees é mais tolerante)\n",
        "    min_samples_leaf=1,         # Menor que RF (Extra Trees é mais tolerante)\n",
        "    max_features='sqrt',        # Usa sqrt(n_features) para cada split\n",
        "    bootstrap=True,             # Bootstrap para diversidade\n",
        "    class_weight='balanced',    # Lida com desbalanceamento\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Ajustando os hiperparâmetros do LogisticRegression para maximizar a acurácia\n",
        "lr = LogisticRegression(\n",
        "    C=1.5,  # Aumentando C para menos regularização, geralmente melhora a acurácia\n",
        "    solver='lbfgs',  # Solver eficiente para conjuntos de dados não esparsos\n",
        "    penalty='l2',  # Regularização padrão\n",
        "    class_weight='balanced',\n",
        "    max_iter=3000,  # Mais iterações para garantir convergência\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# SVM ajustado para acurácia máxima\n",
        "svm = SVC(\n",
        "    C=2.0,                  # Menos regularização\n",
        "    kernel='rbf',\n",
        "    gamma='scale',          # Gamma padrão\n",
        "    class_weight='balanced',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# KNN ajustado para acurácia máxima\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    weights='distance',\n",
        "    metric='minkowski',\n",
        "    p=2                     # Distância Euclidiana\n",
        ")\n",
        "\n",
        "# Ensemble Voting Classifier ajustado\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('gb', gb),\n",
        "        ('et', et),\n",
        "        ('lr', lr),\n",
        "        ('svm', svm),\n",
        "        ('knn', knn)\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[2,2,2,1,1,1]   # Dá mais peso para ensembles de árvore\n",
        ")\n",
        "\n",
        "# Bagging com o melhor modelo individual (Random Forest)\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=rf,\n",
        "    n_estimators=20,        # Mais estimadores para maior estabilidade\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "models = {\n",
        "    'rf': rf,\n",
        "    'gb': gb,\n",
        "    'et': et,\n",
        "    'lr': lr,\n",
        "    'svm': svm,\n",
        "    'knn': knn,\n",
        "    'voting': voting_clf,\n",
        "    'bagging': bagging_clf\n",
        "}\n",
        "\n",
        "print(\"Modelos configurados para máxima acurácia:\")\n",
        "for name, model in models.items():\n",
        "    print(f\"- {name}: {type(model).__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validação cruzada e seleção do melhor modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nVALIDAÇÃO CRUZADA (score: accuracy)\")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_model = None\n",
        "best_score = 0\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    scores = cross_val_score(pipeline, X_engineered, y, cv=cv, scoring='accuracy')\n",
        "    mean_score = scores.mean()\n",
        "    std_score = scores.std()\n",
        "\n",
        "    print(f\"{name}: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
        "\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_model = name\n",
        "\n",
        "print(f\"\\nMelhor modelo: {best_model} com score: {best_score:.4f}\")\n",
        "\n",
        "# Treina o melhor modelo\n",
        "best_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', models[best_model])\n",
        "])\n",
        "\n",
        "best_pipeline.fit(X_engineered, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sistemas de votação (ensembles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble com todos os modelos individuais e melhores modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Ensemble com todos os modelos individuais\n",
        "all_individual_models = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('gb', gb),\n",
        "        ('et', et),\n",
        "        ('lr', lr),\n",
        "        ('svm', svm),\n",
        "        ('knn', knn)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# 2. Ensemble com os melhores modelos (baseado na validação cruzada)\n",
        "# Evita conflito de nomes: não usar 'voting' como apelido\n",
        "# (pois 'voting' é parâmetro do VotingClassifier)\n",
        "\n",
        "top_ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf',       models['rf']),\n",
        "        ('gb',       models['gb']),\n",
        "        ('et',       models['et']),\n",
        "        ('voting_c', models['voting']),  # apelido seguro\n",
        "        ('bagging',  models['bagging'])\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Sanity check\n",
        "assert not isinstance(all_individual_models, tuple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensembles híbridos e ponderados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Ensemble híbrido (combina todos + bagging)\n",
        "hybrid_ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('all_individuals', all_individual_models),\n",
        "        ('top_models', top_ensemble),\n",
        "        ('bagging', bagging_clf)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# 4. Ensemble com pesos baseados na performance\n",
        "weighted_ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('gb', gb),\n",
        "        ('et', et),\n",
        "        ('voting_c', voting_clf),\n",
        "        ('bagging', bagging_clf)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Dicionário com todos os ensembles\n",
        "ensemble_models = {\n",
        "    'all_individuals': all_individual_models,\n",
        "    'top_models': top_ensemble,\n",
        "    'hybrid': hybrid_ensemble,\n",
        "    'weighted': weighted_ensemble\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliação de ensembles e treinamento final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste e seleção do melhor ensemble\n",
        "print(\"Testando diferentes sistemas de votação:\")\n",
        "for name, ensemble in ensemble_models.items():\n",
        "    if isinstance(ensemble, tuple):  # fallback defensivo\n",
        "        ensemble = ensemble[0]\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', ensemble)\n",
        "    ])\n",
        "    scores = cross_val_score(pipeline, X_engineered, y, cv=cv, scoring='accuracy', error_score='raise')\n",
        "    mean_score = scores.mean()\n",
        "    std_score = scores.std()\n",
        "    print(f\"{name}: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_model = name\n",
        "        best_pipeline = pipeline\n",
        "print(f\"\\nMelhor sistema de votação: {best_model} com score: {best_score:.4f}\")\n",
        "# Treina o melhor ensemble (ou melhor pipeline anterior)\n",
        "if 'best_pipeline' in locals():\n",
        "    best_pipeline.fit(X_engineered, y)\n",
        "else:\n",
        "    print(\"Nenhum ensemble superou o melhor modelo individual. Usando pipeline anterior.\")\n",
        "    best_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', models[best_model])\n",
        "    ])\n",
        "    best_pipeline.fit(X_engineered, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predição em teste e submissão\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predições para o conjunto de teste\n",
        "pred_test = best_pipeline.predict(test_engineered)\n",
        "\n",
        "print(f\"\\n   PREDIÇÕES   \")\n",
        "print(f\"Distribuição das predições: {np.bincount(pred_test)}\")\n",
        "\n",
        "# Gera arquivo de submissão\n",
        "submission = pd.read_csv('data/sample_submission.csv')\n",
        "submission['labels'] = pred_test\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Arquivo de submissão salvo como 'submission.csv'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
